{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7df8f4",
   "metadata": {},
   "source": [
    "# Zadanie 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d7f7d",
   "metadata": {},
   "source": [
    "## Ładowanie bibliotek oraz ładowanie danych wraz z ich przygotowaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64f5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "def load_text_data_and_clean():  # and w nazwie sugeruje, że należy tę funkcję rozbić + gramatycznie jest \"load and clean text data\"\n",
    "    \"\"\"Funkcja ładująca surowe dane tekstowe z zadania nr 7 z kodowaniem ISO-8859-1 oraz usuwająca zbędne informacje\"\"\"\n",
    "    dictionary = {'TEXT':[], 'LANGUAGE':[]}\n",
    "    files = ['english.txt', 'german.txt', 'spanish.txt', 'polish.txt', 'finnish.txt', 'italian.txt']\n",
    "    for filename in files:\n",
    "        try:\n",
    "            lang = filename.strip(\".txt\")\n",
    "            with open(filename, encoding='iso-8859-1', mode='r', newline='\\n') as f:\n",
    "                for line in f:\n",
    "                    if line == \"\\n\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        line = line.lower()\n",
    "                        line = re.sub(r'[\\t]+|[\\n]+|[\\r]+|[0-9]+|[^\\w+\\s]', '', line)\n",
    "                        dictionary['TEXT'].append(line)\n",
    "                        dictionary['LANGUAGE'].append(lang)\n",
    "        except IOError:\n",
    "            print(f\"Plik {lang} nie został odnalezniony\")\n",
    "    df = pd.DataFrame(dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25113daf",
   "metadata": {},
   "source": [
    "## Podział danych na zbiór treningowy i testowy w stosunku $0.2$, $0.3$, $0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5b8bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_text_data_and_clean()\n",
    "\n",
    "train_02, test_02 = train_test_split(data, test_size = 0.2, shuffle=True, random_state=123)\n",
    "train_03, test_03 = train_test_split(data, test_size = 0.3, shuffle=True, random_state=123)\n",
    "train_04, test_04 = train_test_split(data, test_size = 0.4, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed00f1a",
   "metadata": {},
   "source": [
    "## Reprezentujemy tekst przy użyciu n-gramów, wykonujemy transformacje i dopasowujemy MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d684ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(3, 3))),\n",
       "                ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_MNB_02_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()), # sprawdzał Pan co robi TfidfTransformer?\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_02_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))), # a nie dałoby się tego jakąś pętlą załatwić?\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_02_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_03_unigram = Pipeline([  # identico jak clf_MNB_02_unigram\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_03_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_03_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_04_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_04_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_04_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()) ])\n",
    "\n",
    "clf_MNB_02_unigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_MNB_02_bigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_MNB_02_trigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_MNB_03_unigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_MNB_03_bigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_MNB_03_trigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_MNB_04_unigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_MNB_04_bigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_MNB_04_trigram.fit(train_04['TEXT'], train_04['LANGUAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0e01a",
   "metadata": {},
   "source": [
    "## Predykcje dla MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0ae6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clf_MNB_02_unigram = clf_MNB_02_unigram.predict(test_02['TEXT'])\n",
    "predicted_clf_MNB_02_bigram = clf_MNB_02_bigram.predict(test_02['TEXT'])\n",
    "predicted_clf_MNB_02_trigram = clf_MNB_02_trigram.predict(test_02['TEXT'])\n",
    "predicted_clf_MNB_03_unigram = clf_MNB_03_unigram.predict(test_03['TEXT'])\n",
    "predicted_clf_MNB_03_bigram = clf_MNB_03_bigram.predict(test_03['TEXT'])\n",
    "predicted_clf_MNB_03_trigram = clf_MNB_03_trigram.predict(test_03['TEXT'])\n",
    "predicted_clf_MNB_04_unigram = clf_MNB_04_unigram.predict(test_04['TEXT'])\n",
    "predicted_clf_MNB_04_bigram = clf_MNB_04_bigram.predict(test_04['TEXT'])\n",
    "predicted_clf_MNB_04_trigram = clf_MNB_04_trigram.predict(test_04['TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec979a76",
   "metadata": {},
   "source": [
    "## Precision, recall, f1 i accuracy dla MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf738fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     english       1.00      0.99      1.00      6667\n",
      "     finnish       1.00      1.00      1.00      6255\n",
      "      german       0.95      1.00      0.98     13710\n",
      "     italian       1.00      0.83      0.91      2667\n",
      "      polish       1.00      0.19      0.32       246\n",
      "     spanish       0.99      1.00      1.00      6252\n",
      "\n",
      "    accuracy                           0.98     35797\n",
      "   macro avg       0.99      0.84      0.87     35797\n",
      "weighted avg       0.98      0.98      0.98     35797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_MNB_02_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0b7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_MNB_02_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b42e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_MNB_02_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e47da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_MNB_03_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baebf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_MNB_03_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a286e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_MNB_03_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cde9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_MNB_04_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e89b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_MNB_04_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3244eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_MNB_04_trigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51ad19",
   "metadata": {},
   "source": [
    "## Reprezentujemy tekst przy użyciu n-gramów, wykonujemy transformacje i dopasowujemy regresję logistyczną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc89172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR_02_unigram = Pipeline([  # zauważam pewne podobieństwo do komórki nieco wyżej - od czegoś są funkcje\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_02_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_02_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_03_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_03_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_03_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_04_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_04_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_04_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(solver='newton-cg', max_iter=100)) ])\n",
    "\n",
    "clf_LR_02_unigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_LR_02_bigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_LR_02_trigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_LR_03_unigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_LR_03_bigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_LR_03_trigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_LR_04_unigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_LR_04_bigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_LR_04_trigram.fit(train_04['TEXT'], train_04['LANGUAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30d41e",
   "metadata": {},
   "source": [
    "## Predykcje dla regresji logistycznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ede305",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clf_LR_02_unigram = clf_LR_02_unigram.predict(test_02['TEXT'])\n",
    "predicted_clf_LR_02_bigram = clf_LR_02_bigram.predict(test_02['TEXT'])\n",
    "predicted_clf_LR_02_trigram = clf_LR_02_trigram.predict(test_02['TEXT'])\n",
    "predicted_clf_LR_03_unigram = clf_LR_03_unigram.predict(test_03['TEXT'])\n",
    "predicted_clf_LR_03_bigram = clf_LR_03_bigram.predict(test_03['TEXT'])\n",
    "predicted_clf_LR_03_trigram = clf_LR_03_trigram.predict(test_03['TEXT'])\n",
    "predicted_clf_LR_04_unigram = clf_LR_04_unigram.predict(test_04['TEXT'])\n",
    "predicted_clf_LR_04_bigram = clf_LR_04_bigram.predict(test_04['TEXT'])\n",
    "predicted_clf_LR_04_trigram = clf_LR_04_trigram.predict(test_04['TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7820f",
   "metadata": {},
   "source": [
    "## Precision, recall, f1 i accuracy dla regresji logistycznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_LR_02_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_LR_02_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4341f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_LR_02_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_LR_03_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_LR_03_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_LR_03_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_LR_04_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98eefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_LR_04_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_LR_04_trigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde8dc0",
   "metadata": {},
   "source": [
    "## Reprezentujemy tekst przy użyciu n-gramów, wykonujemy transformacje i dopasowujemy SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVM_02_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_02_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_02_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_03_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_03_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_03_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_04_unigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_04_bigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_04_trigram = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(3,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "clf_SVM_02_unigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_SVM_02_bigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_SVM_02_trigram.fit(train_02['TEXT'], train_02['LANGUAGE'])\n",
    "clf_SVM_03_unigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_SVM_03_bigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_SVM_03_trigram.fit(train_03['TEXT'], train_03['LANGUAGE'])\n",
    "clf_SVM_04_unigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_SVM_04_bigram.fit(train_04['TEXT'], train_04['LANGUAGE'])\n",
    "clf_SVM_04_trigram.fit(train_04['TEXT'], train_04['LANGUAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c82c1f",
   "metadata": {},
   "source": [
    "## Predykcje dla SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748df041",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clf_SVM_02_unigram = clf_SVM_02_unigram.predict(test_02['TEXT'])\n",
    "predicted_clf_SVM_02_bigram = clf_SVM_02_bigram.predict(test_02['TEXT'])\n",
    "predicted_clf_SVM_02_trigram = clf_SVM_02_trigram.predict(test_02['TEXT'])\n",
    "predicted_clf_SVM_03_unigram = clf_SVM_03_unigram.predict(test_03['TEXT'])\n",
    "predicted_clf_SVM_03_bigram = clf_SVM_03_bigram.predict(test_03['TEXT'])\n",
    "predicted_clf_SVM_03_trigram = clf_SVM_03_trigram.predict(test_03['TEXT'])\n",
    "predicted_clf_SVM_04_unigram = clf_SVM_04_unigram.predict(test_04['TEXT'])\n",
    "predicted_clf_SVM_04_bigram = clf_SVM_04_bigram.predict(test_04['TEXT'])\n",
    "predicted_clf_SVM_04_trigram = clf_SVM_04_trigram.predict(test_04['TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39429ba1",
   "metadata": {},
   "source": [
    "## Precision, recall, f1 i accuracy dla SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_SVM_02_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05735d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_SVM_02_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43daa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_02['LANGUAGE'], predicted_clf_SVM_02_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_SVM_03_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dfc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_SVM_03_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf78734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_03['LANGUAGE'], predicted_clf_SVM_03_trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_SVM_04_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_SVM_04_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_04['LANGUAGE'], predicted_clf_SVM_04_trigram))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
